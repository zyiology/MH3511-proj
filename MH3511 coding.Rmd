---
title: "Untitled"
author: "Ancel Chong Fu Shea"
date: "3/16/2023"
output: html_document
---

## R Markdown
Table of Contents
1. Introduction
2. Data description
3. Cleaning datasets
  - 3.1 Summary Stats of each var (up to 3.1.6)
    -> 3.1.1: SAT
      > Hist
      > Boxplot
  - 3.2 Compiled Data set for analysis
    -> 3.2.1 Timeshifted data (not normalised, for modelling)
    -> 3.2.2 ANOVA/T-test data (normalised)
4. Statistical Analysis
  - 4.1 ANOVA
  - 4.2 F-test/t-test
5. Linear regression
6. Conclusion
7. Appendix
8. References
  

  

## Set up
```{r}
library(tidyverse)
library(readxl)
library(dplyr)
library(scales)
library(gghalves)
library(ggplot2)
library(lubridate)
library(gridExtra)
library(ncdf4)
library(tibbletime)
library(GGally)

#check which ones are needed later
library(parallel)
library(neuralnet)
library(randomForest)
library(xgboost)
library(keras)
library(tensorflow)
library(rnn)
library(caret)

library(tseries)
library(forecast)
library(pracma)
library(car)
```

## Reading Data (what data is this)
```{r}


set.seed(999)

data <- read.csv('data/historical-daily-weather-records.csv')

head(data)

data$date<-as.Date(data[,1])

monthly <- group_by(data,month = lubridate::floor_date(date, 'month')) 

```

## Reading SG Surface Air Temp data   
```{r}

sat <- read.csv('data/surface-air-temperature-monthly-mean.csv')

sat[,1]<-as.Date(paste(sat[,1], "-01", sep=""))

#getting reference range..
sat.ref <- sat[sat$month >= as.Date('1980-01-01') & sat$month <= as.Date('2010-12-31'),]

#calculate avg temp/month in ref period to act as a baseline for anomalies..
sat.ref <- sapply(1:12, function(m){
  avg.temp <- round(mean(sat.ref[which(month(sat.ref$month)==m),]$mean_temp),4)
  
})
sat.new <- sat[sat$month >= as.Date('2011-01-01') & sat$month <= as.Date('2022-12-31'),]

#calculating sat anomalies from 2011 onwards..
sat.anom<- data.frame(month = sat.new$month,
                        anom = sapply(1:nrow(sat.new),function(i){
                        
                        #sat on a specific month - sat in ref period of the same month..
                        anomaly <- sat.new$mean_temp[i]-sat.ref[month(sat.new$month[i])]
                          
                        }))


```

```{r}

ggplot(sat) + 
  geom_point(aes(x=month, y = mean_temp)) + 
  geom_line(aes(x=month, y = mean_temp))+
  labs(x= 'Time (Years)', y = 'Temperature (\u00B0C)') +
  theme_minimal() +
  ggtitle("Plot showing the Average monthly surface air temperature against Time")

```

```{r}

ggplot(sat)+
  geom_boxplot(aes(y=mean_temp), width = 0.1) + 
  geom_half_point(aes(y = mean_temp), side ="l",range_scale =0.3)+
  labs(y = 'Temperature (\u00B0C)') +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points for Average monthly surface air temperature") + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```


```{r}

opbins_sat <- 1 + 3.322 * log(nrow(sat))

ggplot(sat)+
  geom_histogram(aes(x=mean_temp),bins = opbins_sat)+
  labs(x = 'Temperature (\u00B0C)', y='Frequency') +
  theme_minimal() +
  ggtitle("Histogram showing the distribution of \nData points for Average monthly surface air temperature")

```



```{r}

qqnorm(sat$mean_temp, pch = 1, frame = FALSE)
qqline(sat$mean_temp, col = "steelblue", lwd = 2)


adf.test(sat$mean_temp) #if p < 0.05, reject, stationary
kpss.test(sat$mean_temp) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: Use differencing for sat

```
From the boxplot, histogram, and qqplot, we can conclude that the dataset for average monthly surface air temperature is normally distributed, and thus does not require any transformation.



## Reading monthly SG rainfall data
```{r}

#reading rainfall data..
rainf <- read.csv('data/rainfall-monthly-total.csv')

rainf[,1]<-as.Date(paste(rainf[,1], "-01", sep=""))

#getting reference range for rainfall..
rainf.ref <- rainf[rainf$month >= as.Date('1980-01-01') & rainf$month <= as.Date('2010-12-31'),]

#calculate avg rainf/month in ref period to act as a baseline for anomalies..
rainf.ref <- sapply(1:12, function(m){
  avg.rainf <- round(mean(rainf.ref[which(month(rainf.ref$month)==m),]$total_rainfall),4)
})

#find rainfall data period of interest, 2011 onwards..
rainf.new <- rainf[rainf$month >= as.Date('2011-01-01') & rainf$month <= as.Date('2022-12-31'),]

#caluclating rainfall anomalies from 2011..
rainf.anom<- data.frame(month = rainf.new$month,
                        anom = sapply(1:nrow(rainf.new),function(i){
                        
                        #rainfall on a specific month - rainfall in ref period of the same month
                        anomaly <- rainf.new$total_rainfall[i]-rainf.ref[month(rainf.new$month[i])]
                          
                        }))

```

## sqrt of SG rainfall data
```{r}

rainfsqrt <- sqrt(rainf$total_rainfall)

rainfsqrt_df <- data.frame(rainfsqrt, month=rainf$month)

#getting reference range for rainfall..
rainfsqrt.ref <- rainfsqrt_df[rainfsqrt_df$month >= as.Date('1980-01-01') & rainfsqrt_df$month <= as.Date('2010-12-31'),]

#calculate avg rainf/month in ref period to act as a baseline for anomalies..
rainfsqrt.ref <- sapply(1:12, function(m){
  avg.rainf <- round(mean(rainfsqrt.ref[which(month(rainfsqrt.ref$month)==m),]$rainfsqrt),4)
})

#find rainfall data period of interest, 2011 onwards..
rainfsqrt.new <- rainfsqrt_df[rainfsqrt_df$month >= as.Date('2011-01-01') & rainfsqrt_df$month <= as.Date('2022-12-31'),]

#caluclating rainfall anomalies from 2011..
rainfsqrt.anom<- data.frame(month = rainfsqrt.new$month,
                        anom = sapply(1:nrow(rainfsqrt.new),function(i){
                        
                        #rainfall on a specific month - rainfall in ref period of the same month
                        anomaly <- rainfsqrt.new$rainfsqrt[i]-rainfsqrt.ref[month(rainfsqrt.new$month[i])]
                          
                        }))

```

```{r}

ggplot(rainf) + 
  geom_point(aes(x=month, y = total_rainfall)) + 
  geom_line(aes(x=month, y = total_rainfall)) +
  labs(x= 'Time (Years)', y = 'Rainfall (mm)') +
  theme_minimal() +
  ggtitle("Plot showing Total monthly rainfall against Time")

adf.test(rainf$total_rainfall) #if p < 0.05, reject, stationary
kpss.test(rainf$total_rainfall) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: Use differencing for rainf

```


```{r}
ggplot(rainf)+
  geom_boxplot(aes(y=total_rainfall), width = 0.1) + 
  geom_half_point(aes(y = total_rainfall), side ="l",range_scale =0.3)+
  labs(y = 'Rainfall (mm)') +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points for Total monthly rainfall")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

```{r}
opbins_rainf <- 1 + 3.322 * log(nrow(rainf))

ggplot(rainf)+
  geom_histogram(aes(x=total_rainfall),bins = opbins_rainf)+
  labs(x='Rainfall (mm)', y = 'Frequency') +
  theme_minimal() +
  ggtitle("Histogram showing the distribution of \nData points for Total monthly rainfall")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

```{r}

qqnorm(rainf$total_rainfall, pch = 1, frame = FALSE)
qqline(rainf$total_rainfall, col = "steelblue", lwd = 2)


```
From the boxplot, histogram, and qqplot, we can observe a distinct right skewed distribution of the datapoints, indicating the presence of very large outliers. We can consider excluding these points or subjecting the dataset to a log or sqrt transformation. 


## SST Summary Statistics for SST
```{r}

#read sst data
sst <- read.table('data/sstoi.indices.txt',header=T)

sst.n34 <- data.frame(month = as.Date(paste(sst$YR,sst$MON,'01',sep='-')),
                        sst = sst$NINO3.4)

sst.n34 <- sst.n34[!duplicated(sst.n34$month),]

sst.hist <- ggplot(data=sst.n34, aes(x=sst))+
  geom_histogram(color='black',bins=50)+
  xlab('SST')+
  ylab('Frequency')+
  ggtitle('Pacific monthly SST data from Jan 1982 to Feb 2023')
  
sst.hist #generally unimodal, normal dist.

boxplot(sst.n34$sst, 
        main = 'Pacific monthly SST data from Jan 1982 to Feb 2023',
        ylab = 'SST')

sst.ts <- ggplot(sst.n34, aes(x=month,y=sst))+
  geom_path()+
  ggtitle('Pacific monthly SST Time Series from 1982 to 2023')+
  xlab('Time')

sst.ts #data looks sound

ggplot(sst.n34) + 
  geom_point(aes(x=month, y = sst)) + 
  geom_line(aes(x=month, y = sst))

ggplot(sst.n34)+
  geom_boxplot(aes(y=sst), width = 0.1) + 
  geom_half_point(aes(y = sst), side ="l",range_scale =0.3)

opbins_sst <- 1 + 3.322 * log(nrow(sst.n34))

ggplot(sst.n34)+
  geom_histogram(aes(x=sst),bins = opbins_sst)

#qqnorm(sst.n34$sst, pch = 1, frame = FALSE)
#qqline(sst.n34$sst, col = "steelblue", lwd = 2)

adf.test(sst.n34$sst) #if p < 0.05, reject, stationary
kpss.test(sst.n34$sst) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case2: stationary, no mods needed for sst

```

## Finding SST Anomaly Data in the Pacific
```{r}
#finding ref range 1980-2010
sst.ref <- sst[sst$YR >= 1980 & sst$YR <= 2010,]

#sst baseline per month 
sst.ref <- sapply(1:12, function(m){
  avg.sst <- round(mean(sst.ref[which(month(sst.ref$MON)==m),]$NINO3.4),4)
})

#sst range for 2011 onwards, data that matters
sst.new<- sst[sst$YR >= 2011 & sst$YR <= 2022,]

# finding anomaly with fixed dates. assume data was with respect to first day of the month
# note the data has anomaly values bur wrt to 1990-2020.
sst.anom<- data.frame(month = as.Date(paste(sst.new$YR,sst.new$MON,'01',sep='-')),
                        anom = sapply(1:nrow(sst.new),function(i){
                        
                        #sst on a specific month - sst in ref period of the same month
                        anomaly <- sst.new$NINO3.4[i]-sst.ref[sst.new$MON[i]]
                          
                        }))

#remove duplicate data
dupes <- duplicated(sst.anom$month)
sst.anom <- subset(sst.anom,!dupes)
  
```

## Plotting scatter plots of SST with SG SAT and Rainfall
```{r}
df_plot <- data.frame(date = sst.anom$month,
                      sst_anom = sst.anom$anom,
                      sat_anom=sat.anom$anom,
                      rainf_anom=rainf.anom$anom)

#calculate correlation
sst.sat.corr <- cor(df_plot$sst_anom,df_plot$sat_anom)
sst.rainf.corr <- cor(df_plot$sst_anom,df_plot$rainf_anom)

df_corr <- data.frame(sst.sat.corr,sst.rainf.corr)

print(df_corr)

#scatter plots
ggplot(df_plot,aes(x=rainf_anom))+
  geom_point(aes(y=sst_anom,color='sst.rainf'))+
  scale_color_manual(values=c('sst.rainf'='pink'))+
  ggtitle('Monthly SG rainfall anomaly againt SST anomaly from Jan 2011- Dec 2022')

ggplot(df_plot,aes(x=sat_anom))+
  geom_point(aes(y=sst_anom,color='sst.sat'))+
  scale_color_manual(values=c('sst.sat'='lightblue'))+
  ggtitle('Monthly SG SST anomaly againt SST anomaly from Jan 2011- Dec 2022')

# timeshifting to find best correlation for sst & sg sat, consider only forward shift since we using pacific data to predict sg data
corr.ts.sat.sst <- sapply(1:12,function(i){
  
  ts.sst <- sst.anom[i:nrow(sst.anom),2]
  ts.sat <- sat.anom[1:(nrow(sst.anom)-i+1),2]
  cor.ts <- cor(ts.sst,ts.sat)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.sat.sst.df <- data.frame(month = corr.ts.sat.sst[1,],
                              corr.sst.sat = corr.ts.sat.sst[2,])

best.ts.sst.sat <- which.max(abs(corr.ts.sat.sst.df$corr.sst.sat)) -1

# timeshifting to find best correlation for sst & sg rainf, consider only forward shift since we using pacific data to predict sg data
corr.ts.rainf.sst <- sapply(1:12,function(i){
  
  ts.sst <- sst.anom[i:nrow(sst.anom),2]
  ts.rainf <- rainf.anom[1:(nrow(rainf.anom)-i+1),2]
  cor.ts <- cor(ts.sst,ts.rainf)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.rainf.sst.df <- data.frame(month = corr.ts.rainf.sst[1,],
                              corr.sst.rainf = corr.ts.rainf.sst[2,])

best.ts.sst.rainf <- which.max(abs(corr.ts.rainf.sst.df$corr.sst.rainf))

```

## Reading Low level wind data

```{r read_wind_data}

winddata = nc_open('data/wspd.mean.nc')


#calculating baseline values from 1980-2010
time = ncvar_get(winddata,"time") #data starts from 1800/01, end at 2023/01
startMonth = (1980-1800)*12+1 #1980/01
endMonth = (2011-1800)*12 #2010/12

lat = ncvar_get(winddata,'lat')
lon = ncvar_get(winddata,'lon')
latIndices = which(lat<=5 & lat>=-5)
lonIndices = which(lon<=240 & lon>=190)

#3D array, lat, lon, time dimensions
windValues = ncvar_get(winddata, 'wspd')

#get the mean of the wind values within the nino3.4 region
ninoAreaMean = function(index) {
  windslice = windValues[,,index]
  ninoRegionData = windslice[lonIndices,latIndices]
  return(mean(ninoRegionData,na.rm=TRUE))
}

#get the means corresponding to reference period
refPeriodValues = sapply(startMonth:endMonth, ninoAreaMean)

#all wind values in one col
allWind <- sapply(((1980-1800)*12+1):((2023-1800)*12),ninoAreaMean)

#calculate a baseline temperature for each month based on the reference period
monthlyRefMeans = sapply(1:12,function(i) {
  monthValues = refPeriodValues[seq(i,length(refPeriodValues),12)]
  return(mean(monthValues))
})


#calculating anomaly (temperature for a given month - corresponding baseline)

startMonth = (2011-1800)*12+1 #2011/01
endMonth = (2023-1800)*12 #2022/12
periodValues = sapply(startMonth:endMonth, ninoAreaMean)
periodAnomalies = periodValues - rep(monthlyRefMeans,(2023-2011))


datetime_wind <- as.Date(time, origin="1800-01-01")

df_wind <- data.frame(periodAnomalies, month=datetime_wind[2534:2677])

ggplot(df_wind) + 
  geom_point(aes(x=month, y = periodAnomalies)) + 
  geom_line(aes(x=month, y = periodAnomalies))

ggplot(df_wind)+
  geom_boxplot(aes(y=periodAnomalies), width = 0.1) + 
  geom_half_point(aes(y = periodAnomalies), side ="l",range_scale =0.3)

opbins_wind <- 1 + 3.322 * log(nrow(df_wind))

ggplot(df_wind)+
  geom_histogram(aes(x=periodAnomalies),bins = opbins_wind)

#qqnorm(df_wind$periodAnomalies, pch = 1, frame = FALSE)
#qqline(df_wind$periodAnomalies, col = "steelblue", lwd = 2)

adf.test(allWind) #if p < 0.05, reject, stationary
kpss.test(allWind) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: stationary, differencing for wind


```

time shift for wind anomaly and sg sat anomaly
```{r time_shift_wind_anomaly}

cors = rep(0,13)
for (i in 0:12) {
  windanomaly = periodAnomalies[1:(144-i)] #1:144 to 1:132
  sat_anom = sat.anom$anom[(1+i):(144)] #1:144 to 13:144
  cors[i]=cor(windanomaly,sat_anom)
}

corsDF = data.frame(cors = cors, x = 0:12)

ggplot(corsDF) +
  geom_line(aes(y=cors,x=x))

#should use the wind data to predict SG temperature x months in the future
#i.e. jan win data is used to predict x SG temp
timeShiftWindSAT = which.max(abs(cors))


```

```{r}

cors = rep(0,13)
for (i in 0:12) {
  windanomaly = periodAnomalies[1:(144-i)] #1:144 to 1:132
  rainf_anom = rainf.anom$anom[(1+i):(144)] #1:144 to 13:144
  cors[i]=cor(windanomaly,rainf_anom)
}

corsDF = data.frame(cors = cors, x = 0:12)

ggplot(corsDF) +
  geom_line(aes(y=cors,x=x))

#same process as for sat
timeShiftWindRainf = which.max(abs(cors))

```


## Reading Sub Surface Temp data
```{r}

months <- 01:12

years <- 1980:2022

substemp.folder <- 'data/subsurfacetempfiles/'

substemp.file <- paste0(substemp.folder, 'EN.4.2.2.f.analysis.g10.')

#extracting subsurface data from netcdf files
substemp <- lapply(1:length(years),function(i){
  
  monthly <- lapply(1:length(months),function(k){
    
    if(months[k]<10){
      data <- nc_open(paste(substemp.file,years[i],'0',months[k],'.nc',sep=''))
    }
    else{
      data <- nc_open(paste(substemp.file,years[i],months[k],'.nc',sep=''))
    }

    
  })
  
})

#finding the mean temp at each month and each year
substemp.all <- sapply(1:length(substemp),function(j){
  
  monthly <- sapply(1:length(months),function(n){
    
    #finding all the temperature data
    data.temp <- (ncvar_get(substemp[[j]][[n]],'temperature'))
    
    #mean the monthly temp in the area of interest, lon 190-240 (-170,-120), lat 78-88 (-5,5)
    all.temp <- data.temp[190:240,78:88,]
    #temp <- data.temp[190:240,78:88,]
  
})
    
})

substemp.all.df <- data.frame(substemp = c(substemp.all)) #561924 NA values

substemp.all.df <- substemp.all.df - 273.15

substemp.all.df <- data.frame(substemp=na.interp(substemp.all.df)) #interp na values


#opbins_sub <- 1 + 3.322 * log(nrow(substemp.all.df))

#ggplot(data=substemp.all.df, aes(x=substemp))+
#  geom_histogram(color='black',bins=opbins_sub)+
#  xlab('Sub Surf Temp')+
#  ylab('Frequency')+
#  ggtitle('Pacific monthly Sub Surface Temp data from Jan 1980 to Feb 2023')

rm(substemp.all) #remove substemp.all to save space

substemp.hist <- ggplot(data=substemp.all.df, aes(x=substemp))+
  geom_histogram(color='black',bins=100)+
  xlab('Sub Surf Temp')+
  ylab('Frequency')+
  ggtitle('Pacific monthly Sub Surface Temp data from Jan 1980 to Feb 2023')
  
substemp.hist #Super weird shape

boxplot(substemp.all.df$substemp, 
        main = 'Pacific monthly Sub surface temp data from Jan 1980 to Feb 2023',
        ylab = 'Sub Surf Temp')

#finding the mean temp at each month and each year
substemp.mean <- sapply(1:length(substemp),function(j){
  
  monthly <- sapply(1:length(months),function(n){
    
    #finding all the temperature data
    data.temp <- (ncvar_get(substemp[[j]][[n]],'temperature'))
    
    data.temp <- data.temp[190:240,78:88,]
    
    data.temp.interp <- zoo::na.approx(data.temp)
    
    #mean the monthly temp in the area of interest, lon 190-240 (-170,-120), lat 78-88 (-5,5)
    month.temp <- mean(data.temp.interp)
    #temp <- data.temp[190:240,78:88,]
  
})
    
})

rm(substemp) #remove substemp to save space

#convert celsius
substemp.mean <- substemp.mean-273.15

substemp.mean <- c(substemp.mean)

#calculate a baseline temperature for each month based on the reference period
substempRefStartIndex = 1; substempRefEndIndex = (2011-1980)*12
substempMeansRef = sapply(1:12,function(i) {
  monthValues = substemp.mean[seq(i,(2011-1980)*12,12)]
  return(mean(monthValues))
})

substemp.anom = substemp.mean[(substempRefEndIndex+1):length(substemp.mean)] - rep(substempMeansRef,(2023-2011))
  

# previous code - replaced by above
#
# # start from 32 because that corresponds to 2011. Columns are years
# substemp.anom <- sapply(32:ncol(substemp.mean),function(c){
#   
#   substemp.ref <- sapply(1:nrow(substemp.mean),function(r){
#     
#     #index 1:30 for the reference of 1980-2010.
#     anom <- substemp.mean[r,c]-mean(substemp.mean[r,1:30])
#     
#     
#   })
#   
# })

#turn matrix to vector for ease of access
substemp.anom.vec <- as.vector(substemp.anom)

adf.test(substemp.anom.vec) #if p < 0.05, reject, stationary
kpss.test(substemp.anom.vec) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case3: remove trend for substemp

```

## Correlate Sub Surface temp with SG Rainfall & SAT
```{r}
#set upper limit on data so reduce data length
df_plot2 <- data.frame(substemp_anom = substemp.anom.vec,
                      sat_anom=sat.anom[1:144,2],
                      rainf_anom=rainf.anom[1:144,2])

#calculate correlation
subs.sat.corr <- cor(df_plot2$substemp_anom,df_plot2$sat_anom)
subs.rainf.corr <- cor(df_plot2$substemp_anom,df_plot2$rainf_anom)

df_corr2 <- data.frame(subs.sat.corr,subs.rainf.corr)

print(df_corr2)

#scatter plots
ggplot(df_plot2,aes(x=rainf_anom))+
  geom_point(aes(y=substemp_anom,color='subs.rainf'))+
  scale_color_manual(values=c('subs.rainf'='pink'))+
  ggtitle('Monthly Sub Surf Temp anomaly againt SG Rainfall from Jan 2011- Dec 2022')

ggplot(df_plot2,aes(x=sat_anom))+
  geom_point(aes(y=substemp_anom,color='subs.sat'))+
  scale_color_manual(values=c('subs.sat'='lightblue'))+
  ggtitle('Monthly Sub Surf Temp anomaly againt SG SAT anomaly from Jan 2011- Dec 2022')

ggplot(df_plot2,aes(x=1:nrow(df_plot2)))+
  geom_line(aes(y=substemp_anom,color='subs'))+
  geom_line(aes(y=sat_anom,color='sat'))+
  #geom_line(aes(y=rainf_anom,color='rainf'))+
  scale_color_manual(values=c(subs='red',sat='lightgreen','rainf'='lightblue'))

# fakesat <- df_plot2$sat_anom[5:nrow(df_plot2)]
# fakesub <- df_plot2$substemp_anom[1:(nrow(df_plot2)-4)]
# 
# ggplot()+
#   geom_line(aes(x=1:140,y=fakesub,color='subs'))+
#   geom_line(aes(x=1:140,y=fakesat,color='sat'))+
#   scale_color_manual(values=c(subs='red',sat='lightgreen','rainf'='lightblue'))

# timeshifting to find best correlation for substemp & sg sat, consider only forward shift since we using pacific data to predict sg data
corr.ts.sat.substemp <- sapply(1:12,function(i){
  
  ts.substemp <- substemp.anom.vec[i:length(substemp.anom.vec)]
  ts.sat <- sat.anom[1:(nrow(sat.anom)-i+1),2]
  cor.ts <- cor(ts.substemp,ts.sat)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.sat.substemp.df <- data.frame(month = corr.ts.sat.substemp[1,],
                              corr.substemp.sat = corr.ts.sat.substemp[2,])

best.ts.substemp.sat <- which.max(abs(corr.ts.sat.substemp.df$corr.substemp.sat))-1

# timeshifting to find best correlation for substemp & sg rainf, consider only forward shift since we using pacific data to predict sg data
corr.ts.rainf.substemp <- sapply(1:12,function(i){
  
  ts.substemp <- substemp.anom.vec[i:length(substemp.anom.vec)]
  ts.rainf <- rainf.anom[1:(nrow(rainf.anom)-i+1),2]
  cor.ts <- cor(ts.substemp,ts.rainf)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.rainf.substemp.df <- data.frame(month = corr.ts.rainf.substemp[1,],
                              corr.substemp.rainf = corr.ts.rainf.substemp[2,])

best.ts.substemp.rainf <- which.max(abs(corr.ts.rainf.substemp.df$corr.substemp.rainf))
```

```{r}

nc_data <- nc_open("data/olr-monthly_v02r07_197901_202303.nc")

print (nc_data)

attributes(nc_data$var)

attributes(nc_data$dim)

lat <- ncvar_get(nc_data, "lat")

nlat <- dim(lat)

lon <- ncvar_get(nc_data, "lon")

nlon <- dim(lon)

time <- ncvar_get(nc_data, "time")

head(time) 
tunits <- ncatt_get(nc_data, "time", "units") 
nt <- dim(time) 

cloudiness <- ncvar_get(nc_data, "olr") 

fillvalue <- ncatt_get(nc_data, "olr", "_FillValue")

dim(cloudiness)

cloudiness[cloudiness==fillvalue$value] <- NA


#Obs for cloudiness variable: boxplot, histogram, qqplot


# Convert the number of hours to POSIXct format, specifying the origin as "1800-01-01"
datetime <- as.Date(time, origin="1979-01-01")

id.cloud <- which(datetime >= '1982-01-01' & datetime < '2022-01-1')
cloud.data <- cloudiness[1,1,id.cloud]

lat_range <- c(-5, 5)
lon_range <- c(190, 240)

lat_idx <- which(lat >= lat_range[1] & lat <= lat_range[2])
lon_idx <- which(lon >= lon_range[1] & lon <= lon_range[2])

cloud_data <- cloudiness[lon_idx,lat_idx,]

lat_1 <- data.frame(t(cloud_data[1,,]), datetime)

ggplot(lat_1)+
  geom_point(aes(x=datetime, y=X1))+
  geom_line (aes(x=datetime, y=X1))

pacific_avg <- sapply(1:531, function(a){
  year_avg <- mean(cloudiness[,,a])
})

lat <- data.frame(pacific_avg, datetime)

```

```{r}

ggplot(lat)+
  geom_point(aes(x=datetime, y=pacific_avg))+
  geom_line (aes(x=datetime, y=pacific_avg))+
  labs(x= 'Time (Years)', y = 'OLR'~W/m^2) +
  theme_minimal() +
  ggtitle("Plot showing OLR against Time")

adf.test(lat$pacific_avg) #if p < 0.05, reject, stationary
kpss.test(lat$pacific_avg) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: stationary, differencing for wind

```

```{r}

ggplot(lat)+
  geom_boxplot(aes(y=pacific_avg), width = 0.1) + 
  geom_half_point(aes(y = pacific_avg), side ="l",range_scale =0.3)+
  labs(y = 'OLR'~W/m^2) +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points of OLR")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```


```{r}

opbins_lat <- 1 + 3.322 * log(nrow(lat))


test <- lat[385:528,]

ggplot(lat)+
  geom_histogram(aes(x=pacific_avg),bins = opbins_lat)+
  labs(x = 'OLR'~W/m^2, y='Frequency') +
  theme_minimal() +
  ggtitle("Histogram showing the distribution of \nData points of OLR")

```

```{r}


#qqnorm(lat$pacific_avg, pch = 1, frame = FALSE)
#qqline(lat$pacific_avg, col = "steelblue", lwd = 2)
reference <- lat[13:384,]

monthly_means = sapply(1:12, function(i) {
  mean(reference[seq(i,nrow(reference),12),1])
})

test <- lat[385:528,]

test_anomoly = test[,1] - rep(monthly_means,12)


```
From the boxplot, histogram, and qqplot, we can observe a distinct right skewed distribution of the datapoints, indicating the presence of very large outliers. We can consider excluding these points or subjecting the dataset to a log, sqrt, or normalize transformation. 



## All Data, Pairs Plot (Not Time Shifted)
```{r message=FALSE, warning=FALSE}
enso.df <- data.frame(month = sat.anom$month, 
                      sat.sg=sat.anom$anom, # Normally distributed
                      rainf.sg=rainf.anom$anom, #
                      sst.pcf=sst.anom$anom,
                      substemp.pcf=substemp.anom.vec,
                      okla.pcf=test_anomoly,
                      wind.pcf=periodAnomalies)

enso.df.nd <- enso.df[,2:ncol(enso.df)]

lowerFn <- function(data, mapping, method = "lm", ...) {
  p <- ggplot(data = data, mapping = mapping) +
    geom_point(colour = "black",size=0.3,alpha=0.5) +
    geom_smooth(method = method, color = "red", alpha=0.7,linewidth=0.5,...)
  p
}

ggpairs(enso.df.nd,columns = 1:ncol(enso.df.nd),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))

#Stat tests
enso.df.t1 <- enso.df[which(enso.df$month < '2017-01-01'),] #half of data
enso.df.t2 <- enso.df[which(enso.df$month >='2017-01-01'),]

#t test for anomaly
t.test.results <- sapply(2:ncol(enso.df.t1),function(i){
  
  t.test(enso.df.t1[,i],enso.df.t2[,i],paired=F)})

colnames(t.test.results) <- colnames(enso.df.nd)

#ANOVA
#all data for 1982-2022 only.
all.data <- data.frame(month = sat[1:480,1],
                      sat.sg=sat[1:480,2],
                      rainf.sg=rainf[1:480,2],
                      sst.pcf=sst.n34[1:480,2],
                      substemp.pcf=substemp.mean[25:504],
                      okla.pcf=cloud.data,
                      wind.pcf=allWind[25:504])

all.data$decade <- factor(c(rep(1,120),
                         rep(2,120),
                         rep(3,120),
                         rep(4,120)))

var.sat<-leveneTest(sat.sg~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.sat <- summary(aov(sat.sg ~ decade, data=all.data))
p.sat <- aov.sat[[1]]$`Pr(>F)`[1]

var.rainf<-leveneTest(rainf.sg~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.rainf <- summary(aov(rainf.sg ~ decade, data=all.data))
p.rainf <- aov.rainf[[1]]$`Pr(>F)`[1]

var.sst<-leveneTest(sst.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.sst <- summary(aov(sst.pcf ~ decade, data=all.data))
p.sst <- aov.sst[[1]]$`Pr(>F)`[1]

var.substemp<-leveneTest(substemp.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.substemp <- summary(aov(substemp.pcf ~ decade, data=all.data))
p.substemp <- aov.substemp[[1]]$`Pr(>F)`[1]

var.okla<-leveneTest(okla.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.okla <- summary(aov(okla.pcf ~ decade, data=all.data))
p.okla <- aov.okla[[1]]$`Pr(>F)`[1]

var.wind<-leveneTest(wind.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.wind <- summary(aov(wind.pcf ~ decade, data=all.data))
p.wind <-aov.wind[[1]]$`Pr(>F)`[1]

# since p values of sat, sst, wind <0.05, we cannot rej H0. ie the means are different across decadal time scales. Therefore, in order to analyse enso trend, we will need more data to capture the variations better as a sample size of 5 decades are clearly insufficient to fully capture the variations.

```

# Correlating cloudiness anomaly of 2011-2023 to Singapore anomaly (Rainfall and temeprature)
#``r
#cor(rainf.anom[,2], test_anomoly)
#cor(sat.anom[,2], test_anomoly)
#cors = rep(0,11)

#for (i in -12:0) {
#  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
#                        sat_anom = sat.anom$anom[(-i+1):nrow(sat.anom)])
#  cors[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$sat_anom))
#}
#
#cors2 = rep(0,11)
#for (i in -12:0) {
#  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
#                        rain_anom = rainf.anom$anom[(-i+1):nrow(rainf.anom)])
#  cors2[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$rain_anom))
#}



## Correlating cloudiness anomaly of 2011-2023 to Singapore anomaly (Rainfall and temeprature)

```{r}

cor(rainf.anom[,2], test_anomoly)

rainf.anom

cor(sat.anom[,2], test_anomoly)

cors = rep(0,11)
for (i in -12:0) {
  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
                        sat_anom = sat.anom$anom[(-i+1):nrow(sat.anom)])
  cors[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$sat_anom))
}

timeShiftCloudSAT = which.max(abs(cors))

cors2 = rep(0,11)
for (i in -12:0) {
  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
                        rain_anom = rainf.anom$anom[(-i+1):nrow(rainf.anom)])
  cors2[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$rain_anom))
}

timeShiftCloudRainf = which.max(abs(cors2))

```
## Timeshifting our data

```{r}

#enso.df

maxshift = max(c(timeShiftWindRainf,
                 timeShiftWindSAT,
                 best.ts.substemp.rainf,
                 #best.ts.substemp.sat,
                 timeShiftCloudSAT,
                 timeShiftCloudRainf))

n = nrow(enso.df)

timeshiftedRainfDF = data.frame(rainf.sg = enso.df$rainf.sg[(1+maxshift):n],
                                sst.pcf = enso.df$sst.pcf[(1+maxshift):n],#not timeshifted
                                substemp.pcf = enso.df$substemp.pcf[(1+(maxshift-best.ts.substemp.rainf)):(n-best.ts.substemp.rainf)],
                                okla.pcf = enso.df$okla.pcf[(1+maxshift-timeShiftCloudRainf):(n-timeShiftCloudRainf)],
                                wind.pcf = enso.df$wind.pcf[(1+maxshift-timeShiftWindRainf):(n-timeShiftWindRainf)])

                                
timeshiftedSATDF = data.frame(sat.sg = enso.df$sat.sg[(1+maxshift):n],
                              sst.pcf = enso.df$sst.pcf[(1+maxshift):n],#not timeshifted,
                              substemp.pcf = enso.df$substemp.pcf[(1+maxshift):n],#not timeshifted
                              okla.pcf = enso.df$okla.pcf[(1+maxshift-timeShiftCloudSAT):(n-timeShiftCloudSAT)],
                              wind.pcf = enso.df$wind.pcf[(1+maxshift-timeShiftWindSAT):(n-timeShiftWindSAT)])
ggpairs(timeshiftedSATDF,columns = 1:ncol(timeshiftedSATDF),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))


ggpairs(timeshiftedRainfDF,columns = 1:ncol(timeshiftedRainfDF),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))

ggpairs(timeshiftedSATDF,columns = 1:ncol(timeshiftedSATDF),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))


ggpairs(timeshiftedRainfDF,columns = 1:ncol(timeshiftedRainfDF),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))

```


```{r, eval=FALSE}


rainFormula = (rainf.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)
satFornula = (sat.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

testIndices = sample(1:136,30)

train = timeshiftedRainfDF[-testIndices,]
test = timeshiftedRainfDF[testIndices,]

train.st = data.frame(scale(train))

means = colMeans(train); sds = apply(train,2,sd)

test.st = data.frame(scale(test,center=means,scale=sds))

model = glm(formula=rainFormula,
    data=train.st)

preds = predict(model,test.st)


with(summary(model), 1 - deviance/null.deviance)

df = data.frame(preds = preds,
           obs = test$rainf.sg)


#parallelize training of multiple models (initial weights are randomized)
numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','train.st'))
nets = parLapply(cl,1:(36), function(i) {
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
})
stopCluster(cl)

rsquares = sapply(nets[1:(length(nets))], function(net) {
  pred = predict(object=net,newdata=test.st)[,1]
  resi = test.st$rainf.sg-pred
  SSR = sum(resi^2)
  resi2 = test.st['rainf.sg']-mean(test.st$rainf.sg)
  SST = sum(resi2^2)
  rsquare = 1-SSR/SST
})

round(rsquares,4)



#rsquarer <- function (x, y) cor(x, y) ^ 2

#rsquarer(pred,test.st$rainf.sg)
```


```{r}

rsquarer = function(obs,pred) {
  tss = sum((obs-mean(obs))^2)
  rss = sum((obs-pred)^2)
  return(1-rss/tss)
}

params <- list(
  objective = "reg:squarederror", # Specify the objective (e.g., regression)
  eta = 0.01, # Learning rate
  max_depth = 5, # Maximum depth of trees
  subsample = 0.8, # Subsampling ratio
  colsample_bytree = 0.8 # Column subsampling ratio
)

rainFormula = (rainf.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

k=5
n=nrow(timeshiftedRainfDF)
groups = cut(1:n,k,label=F)
timeshiftedRainfDF = timeshiftedRainfDF[sample(n),]

numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','timeshiftedRainfDF','groups','rsquarer','xgboost','params','randomForest'))
results = parSapply(cl, 1:k, function(i) {
  
  train = timeshiftedRainfDF[groups!=i,]
  test = timeshiftedRainfDF[groups==i,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=rainFormula,data=train.st)
  pred = predict(model,test.st)
  linearRMSE = sqrt(sum((pred-test.st$rainf.sg)^2)/length(test.st))
  linearRSQ = rsquarer(test.st$rainf.sg,pred)
  
  # net = neuralnet(formula=rainFormula,
  #                 data=train.st,
  #                 #act.fct='logistic',
  #                 hidden=c(6),
  #                 algorithm="sag",
  #                 stepmax=1e+08,
  #                 linear.output=FALSE)
  # 
  # pred = predict(object=net,newdata=test.st)[,1]
  # nnRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # nnRSQ = rsquarer(test.st$rainf.sg,pred)
  # 
  # xgb = xgboost(data = as.matrix(train.st[,2:5]), label = train.st[,1], params = params,  nrounds = 100)
  # pred = predict(xgb,as.matrix(test.st[,2:5]))
  # xgbRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # xgbRSQ = rsquarer(test.st$rainf.sg,pred)
  
  # rf = randomForest(rainFormula,data=train.st)
  # pred = predict(rf,newdata=test.st)
  # rfRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # rfRSQ = rsquarer(test.st$rainf.sg,pred)

  return(c(linearRMSE,linearRSQ))#,nnRMSE,nnRSQ,rfRMSE,rfRSQ,xgbRMSE,xgbRSQ))
})

results = data.frame(t(results))
colnames(results) = c('linearrmse','linearrsq')#,'nnrmse','nnrsq','rfRMSE','rfRSQ','xgbRMSE','xgbRSQ')
colMeans(results)

```


```{r}

satFormula = (sat.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

timeshiftedSATDF = timeshiftedSATDF[sample(n),]
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','satFormula','timeshiftedSATDF','groups','randomForest','xgboost','params','rsquarer'))
results = parSapply(cl, 1:k, function(i) {
  train = timeshiftedSATDF[groups!=i,]
  test = timeshiftedSATDF[groups==i,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=satFormula,data=train.st)
  pred = predict(model,test.st)
  #linearRMSE = sqrt((pred-test.st$sat.sg)^2)
  linearRMSE = sqrt(sum((pred-test.st$sat.sg)^2)/length(test.st))
  linearRSQ = rsquarer(test.st$sat.sg,pred)
  #linearResult = with(summary(model), 1 - deviance/null.deviance)
  
  net = neuralnet(formula=satFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
  
  pred = predict(object=net,newdata=test.st)[,1]
  nnRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  nnRSQ = rsquarer(test.st$sat.sg,pred)
  
  xgb = xgboost(data = as.matrix(train.st[,2:5]), label = train.st[,1], params = params,  nrounds = 100)
  pred = predict(xgb,as.matrix(test.st[,2:5]))
  xgbRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  xgbRSQ = rsquarer(test.st$sat.sg,pred)
  
  rf = randomForest(satFormula,data=train.st)
  pred = predict(rf,newdata=test.st)
  rfRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  rfRSQ = rsquarer(test.st$sat.sg,pred)
  
  rnn = trainr(Y=train.st[,1],X=train.st[,2:5],learningrate=1)
  pred = predictr(rnn,X=test.st[,2:5])
  rfRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  rfRSQ = rsquarer(test.st$sat.sg,pred)
  

  return(c(linearRMSE,linearRSQ,nnRMSE,nnRSQ,rfRMSE,rfRSQ,xgbRMSE,xgbRSQ))
})

results = data.frame(t(results))
colnames(results) = c('linearrmse','linearrsq','nnrmse','nnrsq','rfRMSE','rfRSQ','xgbRMSE','xgbRSQ')
colMeans(results)

```
## Buiilding a Neural Net
```{r, eval=FALSE}

library(parallel)
library(neuralnet)

rainFormula = (rainf.sg ~ sst.pcf+substemp.pcf+okla.pcf+wind.pcf)
satFormula = (sat.sg ~ sst.pcf+substemp.pcf+okla.pcf+wind.pcf)

train = timeshiftedRainfDF[1:100,]
test = timeshiftedRainfDF[101:136,]

train.st = data.frame(scale(train))

means = colMeans(train); sds = apply(train,2,sd)

test.st = scale(test,center=means,scale=sds)

model = glm(formula=rainFormula,
    data=train)

preds = predict(model,test)

df = data.frame(preds = preds,
           obs = test$rainf.sg)


#parallelize training of multiple models (initial weights are randomized)
numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','train.st'))
nets = parLapply(cl,1:(12), function(i) {
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=5,
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=F)
})
stopCluster(cl)

test.st <- data.frame(test.st)

rsq.fn <- function (x, y) cor(x, y) ^ 2

rsq.net = sapply(nets[1:(length(nets))], function(net) {
  pred = predict(object=net,newdata=test.st)[,1]
  rsq.fn(test.st$rainf.sg,pred)
  
})


rsq.glm <- rsq1(test.st$rainf.sg,preds)


train = timeshiftedRainfDF[groups!=1,]
  test = timeshiftedRainfDF[groups==1,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=rainFormula,data=train.st)
  pred = predict(model,test.st)
  linearRMSE = sqrt(sum((pred-test.st$rainf.sg)^2)/length(test.st))
  
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4,4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
  
  pred = predict(object=net,newdata=test.st)[,1]
  
## RNN
  
rmax_len <- 6 # the number of previous examples we'll look at
n = nrow(timeshiftedRainfDF)

start_indexes = seq(1,n-max_len+1,by=1)

X1 = t(sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),2])))
X2 = t(sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),3])))
X3 = t(sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),4])))
X4 = t(sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),5])))
X = data.frame(cbind(X1,X2,X3,X4))

Y = data.frame(Y=timeshiftedRainfDF[6:n,1])

df = cbind(Y,X)

train = df[1:100,]
test = df[101:131,]
  
train.st = data.frame(scale(train))
means = colMeans(train); sds = apply(train,2,sd)
test.st = data.frame(scale(test,center=means,scale=sds))

net = neuralnet(formula = Y~.,
          data = df,
          hidden=c(6),
          algorithm="rprop+",
          stepmax=1e+08,
          linear.output=TRUE)

pred = predict(object=net,newdata=test.st)[,1]

compare = data.frame(pred=pred,obs=test$Y)

## RNN END


Y = array(,dim=c(6,129,1))
Y[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),1]))

rnn = trainr(Y=Y,X=X,learningrate=1)
pred = predictr(rnn,X=X)


X = array(,dim=c(6,129,1))
X[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),2]))

Y = array(,dim=c(6,129,1))
Y[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),1]))

rnn = trainr(Y=Y,X=X,learningrate=1,numepochs=10)
pred = predictr(rnn,X=X)



```


```{r enso_test}

#monthly data from 1/1979 to 03/2023
ensoIndex = read.csv('data/meiv2.csv',sep=",",header=F)
startIndex = (1982-1979)*12+1; endIndex = (2023-1979)*12

#i want only 1/1982 to 12/2022
ensoIndex = ensoIndex[startIndex:endIndex]

ensoPos = ensoIndex>=0
ensoNeg = ensoIndex<0


#find enso variable: split sg rainfall and SAT into 2 groups based on enso>0 or enso<0

ensoPos_sgRainf = rainf.anom$anom[ensoPos]
ensoNeg_sgRainf = rainf.anom$anom[ensoNeg]

ensoPos_sgSAT = sat.anom$anom[ensoPos]
ensoNeg_sgSAT = sat.anom$anom[ensoNeg]

#then welch 2-sample test to see if got effect
var.test(ensoPos_sgRainf, ensoNeg_sgRainf)

#p-value ~0.5, can be considered same variance?
t.test(ensoPos_sgRainf, ensoNeg_sgRainf,var.equal=TRUE)

#then welch 2-sample test to see if got effect
var.test(ensoPos_sgSAT, ensoNeg_sgSAT)

#p-value ~0.5, can be considered same variance?
t.test(ensoPos_sgSAT, ensoNeg_sgSAT,var.equal=FALSE)



# 
# 
# ensoPos = ensoIndex>=0.3
# ensoNeg = ensoIndex<=-0.3
# 
# 
# #find enso variable: split sg rainfall and SAT into 2 groups based on enso>0 or enso<0
# 
# ensoPos_sgRainf = rainf.anom$anom[ensoPos]
# ensoNeg_sgRainf = rainf.anom$anom[ensoNeg]
# 
# ensoPos_sgSAT = sat.anom$anom[ensoPos]
# ensoNeg_sgSAT = sat.anom$anom[ensoNeg]
# 
# # histogram(ensoPos_sgRainf)
# 
# 
# #then welch 2-sample test to see if got effect
# var.test(ensoPos_sgRainf, ensoNeg_sgRainf)
# 
# #p-value ~0.5, can be considered same variance?
# t.test(ensoPos_sgRainf, ensoNeg_sgRainf,var.equal=FALSE)
# 
# 
# #then welch 2-sample test to see if got effect
# var.test(ensoPos_sgSAT, ensoNeg_sgSAT)
# 
# #p-value ~0.5, can be considered same variance?
# t.test(ensoPos_sgSAT, ensoNeg_sgSAT,var.equal=FALSE)

```

