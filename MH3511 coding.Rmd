---
title: "Untitled"
author: "Ancel Chong Fu Shea"
date: "3/16/2023"
output: html_document
---

## R Markdown
Table of Contents
1. Introduction
2. Data description
3. Cleaning datasets
  - 3.1 Summary Stats of each var (up to 3.1.6)
    -> 3.1.1: SAT
      > Hist
      > Boxplot
  - 3.2 Compiled Data set for analysis
    -> 3.2.1 Timeshifted data (not normalised, for modelling)
    -> 3.2.2 ANOVA/T-test data (normalised)
4. Statistical Analysis
  - 4.1 ANOVA
  - 4.2 F-test/t-test
5. Linear regression
6. Conclusion
7. Appendix
8. References
  

  

## Set up
```{r}
library(tidyverse)
library(readxl)
library(dplyr)
library(scales)
library(gghalves)
library(ggplot2)
library(lubridate)
library(gridExtra)
library(ncdf4)
library(tibbletime)
library(GGally)

#check which ones are needed later
library(parallel)
library(neuralnet)
library(randomForest)
library(xgboost)
library(keras)
library(tensorflow)
library(rnn)
library(caret)

library(tseries)
library(forecast)
library(pracma)
library(car)
```

## Reading Data (what data is this)
```{r}

set.seed(999)
# 
# data <- read.csv('data/historical-daily-weather-records.csv')
# 
# head(data)
# 
# data$date<-as.Date(data[,1])
# 
# monthly <- group_by(data,month = lubridate::floor_date(date, 'month')) 

```

## Reading SG Surface Air Temp data   
```{r}
#creating a time vector for the period of data we will use. 1982-2022 (41 years)
time.all <- seq(as.Date('1982-01-01'),as.Date('2022-12-01'),by='month')

#creating a time vector for the study period we will use. 2011-2022 (12 years)
time.study <- seq(as.Date('2011-01-01'),as.Date('2022-12-01'),by='month')

#creating a time vector for the reference period we will use. 1982-2010 (29 years)
time.ref <- seq(as.Date('1982-01-01'),as.Date('2010-12-01'),by='month')

sat.all <- read.csv('data/surface-air-temperature-monthly-mean.csv')

sat.all[,1]<-as.Date(paste(sat.all[,1], "-01", sep=""))

#getting reference range..
sat.refVal <- sat.all$mean_temp[sat.all$month >= as.Date('1982-01-01') & sat.all$month <= as.Date('2010-12-31')]

#calculate avg temp/month in ref period to act as a baseline for anomalies..
sat.baseline <- sapply(1:12, function(i){
  #avg.temp <- round(mean(sat.ref[which(month(sat.ref$month)==m),]$mean_temp),4)
  sat.refVal[seq(i,length(sat.refVal),12)] %>% mean()
})

sat.studyVal <- sat.all$mean_temp[sat.all$month >= as.Date('2011-01-01') & sat.all$month <= as.Date('2022-12-31')]

#calculating sat anomalies from 2011 onwards..
sat.studyAnom = sat.studyVal - rep(sat.baseline,(2023-2011))

# <- data.frame(month = sat.new$month,
#                         anom = sapply(1:nrow(sat.new),function(i){
#                         
#                         #sat on a specific month - sat in ref period of the same month..
#                         anomaly <- sat.new$mean_temp[i]-sat.ref[month(sat.new$month[i])]
#                           
#                         }))


```

```{r}

ggplot(sat.all) + 
  geom_point(aes(x=month, y = mean_temp)) + 
  geom_line(aes(x=month, y = mean_temp))+
  labs(x= 'Time (Years)', y = 'Temperature (\u00B0C)') +
  theme_minimal() +
  ggtitle("Plot showing the Average monthly surface air temperature against Time")

```

```{r}

ggplot(sat.all)+
  geom_boxplot(aes(y=mean_temp), width = 0.1) + 
  geom_half_point(aes(y = mean_temp), side ="l",range_scale =0.3)+
  labs(y = 'Temperature (\u00B0C)') +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points for Average monthly surface air temperature") + 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```


```{r}

sat.opbins <- 1 + 3.322 * log(nrow(sat.all))

ggplot(sat.all)+
  geom_histogram(aes(x=mean_temp),bins = sat.opbins)+
  labs(x = 'Temperature (\u00B0C)', y='Frequency') +
  theme_minimal() +
  ggtitle("Histogram showing the distribution of \nData points for Average monthly surface air temperature")

```



```{r}

qqnorm(sat.all$mean_temp, pch = 1, frame = FALSE)
qqline(sat.all$mean_temp, col = "steelblue", lwd = 2)


adf.test(sat.all$mean_temp) #if p < 0.05, reject, stationary
kpss.test(sat.all$mean_temp) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: Use differencing for sat

```
From the boxplot, histogram, and qqplot, we can conclude that the dataset for average monthly surface air temperature is normally distributed, and thus does not require any transformation.



## Reading monthly SG rainfall data
```{r}

#reading rainfall data..
rainf.all <- read.csv('data/rainfall-monthly-total.csv')

rainf.all[,1]<-as.Date(paste(rainf.all[,1], "-01", sep=""))

#getting reference range for rainfall..
rainf.refVal <- rainf.all$total_rainfall[rainf.all$month >= as.Date('1982-01-01') & rainf.all$month <= as.Date('2010-12-31')]

#calculate avg rainf/month in ref period to act as a baseline for anomalies..
rainf.baseline<- sapply(1:12, function(i){
  #avg.temp <- round(mean(sat.ref[which(month(sat.ref$month)==m),]$mean_temp),4)
  rainf.refVal[seq(i,length(rainf.refVal),12)] %>% mean()
})

#find rainfall data period of interest, 2011 onwards..
rainf.studyVal <- rainf.all$total_rainfall[rainf.all$month >= as.Date('2011-01-01') & rainf.all$month <= as.Date('2022-12-31')]

#caluclating rainfall anomalies from 2011..
rainf.studyAnom<- rainf.studyVal - rep(rainf.baseline,(2023-2011))

```

## sqrt of SG rainfall data
```{r}

# rainfsqrt <- sqrt(rainf$total_rainfall)
# 
# rainfsqrt_df <- data.frame(rainfsqrt, month=rainf$month)
# 
# #getting reference range for rainfall..
# rainfsqrt.ref <- rainfsqrt_df[rainfsqrt_df$month >= as.Date('1980-01-01') & rainfsqrt_df$month <= as.Date('2010-12-31'),]
# 
# #calculate avg rainf/month in ref period to act as a baseline for anomalies..
# rainfsqrt.ref <- sapply(1:12, function(m){
#   avg.rainf <- round(mean(rainfsqrt.ref[which(month(rainfsqrt.ref$month)==m),]$rainfsqrt),4)
# })
# 
# #find rainfall data period of interest, 2011 onwards..
# rainfsqrt.new <- rainfsqrt_df[rainfsqrt_df$month >= as.Date('2011-01-01') & rainfsqrt_df$month <= as.Date('2022-12-31'),]
# 
# #caluclating rainfall anomalies from 2011..
# rainfsqrt.anom<- data.frame(month = rainfsqrt.new$month,
#                         anom = sapply(1:nrow(rainfsqrt.new),function(i){
#                         
#                         #rainfall on a specific month - rainfall in ref period of the same month
#                         anomaly <- rainfsqrt.new$rainfsqrt[i]-rainfsqrt.ref[month(rainfsqrt.new$month[i])]
#                           
#                         }))

```

```{r}

ggplot(rainf.all) + 
  geom_point(aes(x=month, y = total_rainfall)) + 
  geom_line(aes(x=month, y = total_rainfall)) +
  labs(x= 'Time (Years)', y = 'Rainfall (mm)') +
  theme_minimal() +
  ggtitle("Plot showing Total monthly rainfall against Time")

adf.test(rainf.all$total_rainfall) #if p < 0.05, reject, stationary
kpss.test(rainf.all$total_rainfall) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: Use differencing for rainf

```


```{r}
ggplot(rainf.all)+
  geom_boxplot(aes(y=total_rainfall), width = 0.1) + 
  #geom_half_point(aes(y = total_rainfall), side ="l",range_scale =0.3)+
  labs(y = 'Rainfall (mm)') +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points for Total monthly rainfall")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

```{r}
rainf.opbins <- 1 + 3.322 * log(nrow(rainf.all))

ggplot(rainf.all)+
  geom_histogram(aes(x=total_rainfall),bins = rainf.opbins)+
  labs(x='Rainfall (mm)', y = 'Frequency') +
  theme_minimal() +
  ggtitle("Histogram showing the distribution of \nData points for Total monthly rainfall")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```

```{r}

# qqnorm(rainf.all$total_rainfall, pch = 1, frame = FALSE)
# qqline(rainf.all$total_rainfall, col = "steelblue", lwd = 2)


```
From the boxplot, histogram, and qqplot, we can observe a distinct right skewed distribution of the datapoints, indicating the presence of very large outliers. We can consider excluding these points or subjecting the dataset to a log or sqrt transformation. 


## SST Summary Statistics for SST
```{r}

#read sst data
sst <- read.table('data/sstoi.indices.txt',header=T)

sst.all <- data.frame(month = as.Date(paste(sst$YR,sst$MON,'01',sep='-')),
                        sst = sst$NINO3.4)

sst.all <- sst.all[!duplicated(sst.all$month),]

sst.hist <- ggplot(data=sst.all, aes(x=sst))+
  geom_histogram(color='black',bins=50)+
  xlab('SST')+
  ylab('Frequency')+
  ggtitle('Pacific monthly SST data from Jan 1982 to Feb 2023')
  
sst.hist #generally unimodal, normal dist.

boxplot(sst.all$sst, 
        main = 'Pacific monthly SST data from Jan 1982 to Feb 2023',
        ylab = 'SST')

sst.ts <- ggplot(sst.all, aes(x=month,y=sst))+
  geom_path()+
  ggtitle('Pacific monthly SST Time Series from 1982 to 2023')+
  xlab('Time')

sst.ts #data looks sound

ggplot(sst.all) + 
  geom_point(aes(x=month, y = sst)) + 
  geom_line(aes(x=month, y = sst))

ggplot(sst.all)+
  geom_boxplot(aes(y=sst), width = 0.1) + 
  geom_half_point(aes(y = sst), side ="l",range_scale =0.3)

opbins_sst <- 1 + 3.322 * log(nrow(sst.all))

ggplot(sst.all)+
  geom_histogram(aes(x=sst),bins = opbins_sst)

#qqnorm(sst.n34$sst, pch = 1, frame = FALSE)
#qqline(sst.n34$sst, col = "steelblue", lwd = 2)

adf.test(sst.all$sst) #if p < 0.05, reject, stationary
kpss.test(sst.all$sst) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case2: stationary, no mods needed for sst

```

## Finding SST Anomaly Data in the Pacific
```{r}
#finding ref range 1980-2010
sst.ref <- sst.all[sst.all$month >= '1982-01-01' & sst.all$month < '2011-01-01',]

sst.studyVal <- sst.all[sst.all$month >= '2011-01-01' & sst.all$month < '2023-01-01',2]

sst.refVal <- sst.ref[,2]

#sst baseline per month 
sst.baselines <- sapply(1:12, function(m){
  baseline <- mean(sst.ref[which(month(sst.ref$month)==m),2])
})

# finding anomaly with fixed dates. assume data was with respect to first day of the month
# note the data has anomaly values but wrt to 1990-2020. So we calc our own
sst.studyAnom <- sst.studyVal - rep(sst.baselines,(144/12))
  
```

## Plotting scatter plots of SST with SG SAT and Rainfall
```{r}
df_plot <- data.frame(date = time.study,
                      sst.studyAnom = sst.studyAnom,
                      sat.studyAnom=sat.studyAnom,
                      rainf.studyAnom=rainf.studyAnom)

#calculate correlation
sst.sat.corr <- cor(df_plot$sst.studyAnom,df_plot$sat.studyAnom)
sst.rainf.corr <- cor(df_plot$sst.studyAnom,df_plot$rainf.studyAnom)

df_corr <- data.frame(sst.sat.corr,sst.rainf.corr)

print(df_corr)

#scatter plots
ggplot(df_plot,aes(x=rainf.studyAnom))+
  geom_point(aes(y=sst.studyAnom,color='sst.rainf'))+
  scale_color_manual(values=c('sst.rainf'='pink'))+
  ggtitle('Monthly SG rainfall anomaly againt SST anomaly from Jan 2011- Dec 2022')

ggplot(df_plot,aes(x=sat.studyAnom))+
  geom_point(aes(y=sst.studyVal,color='sst.sat'))+
  scale_color_manual(values=c('sst.sat'='lightblue'))+
  ggtitle('Monthly SG SST anomaly againt SST anomaly from Jan 2011- Dec 2022')

# timeshifting to find best correlation for sst & sg sat, consider only forward shift since we using pacific data to predict sg data
corr.ts.sat.sst <- sapply(1:12,function(i){
  
  ts.sst <- sst.studyAnom[i:length(sst.studyAnom)]
  ts.sat <- sat.studyAnom[1:(length(sat.studyAnom)-i+1)]
  cor.ts <- cor(ts.sst,ts.sat)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.sat.sst.df <- data.frame(month = corr.ts.sat.sst[1,],
                              corr.sst.sat = corr.ts.sat.sst[2,])

sst.timeshift.sat <- which.max(abs(corr.ts.sat.sst.df$corr.sst.sat)) -1

# timeshifting to find best correlation for sst & sg rainf, consider only forward shift since we using pacific data to predict sg data
corr.ts.rainf.sst <- sapply(1:12,function(i){
  
  ts.sst <- sst.studyAnom[i:length(sst.studyAnom)]
  ts.rainf <- rainf.studyAnom[1:(length(rainf.studyAnom)-i+1)]
  cor.ts <- cor(ts.sst,ts.rainf)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.rainf.sst.df <- data.frame(month = corr.ts.rainf.sst[1,],
                              corr.sst.rainf = corr.ts.rainf.sst[2,])

sst.timeshift.rainf <- which.max(abs(corr.ts.rainf.sst.df$corr.sst.rainf))

```

## Reading Low level wind data

```{r read_wind_data}

wind.nc = nc_open('data/wspd.mean.nc')

#3D array, lat, lon, time dimensions
wind.data = ncvar_get(wind.nc, 'wspd')

#calculating baseline values from 1980-2010
wind.time = ncvar_get(wind.nc,"time") #data starts from 1800/01, end at 2023/01
# startMonth = (1980-1800)*12+1 #1980/01
# endMonth = (2011-1800)*12 #2010/12

wind.lat = ncvar_get(wind.nc,'lat')
wind.lon = ncvar_get(wind.nc,'lon')
wind.latIndices = which(wind.lat<=5 & wind.lat>=-5)
wind.lonIndices = which(wind.lon<=240 & wind.lon>=190)

#get the mean of the wind values within the Nino3.4 region
wind.ninoArea = function(index) {
  wind.slice = wind.data[,,index]
  wind.slice = zoo::na.approx(wind.slice)
  return(wind.slice[wind.lonIndices,wind.latIndices])
}

#all wind values
wind.all = lapply(((1982-1800)*12+1):((2023-1800)*12),wind.ninoArea)
wind.allDF = data.frame(values=as.vector(do.call(rbind,wind.all)))

ggplot(data=wind.allDF) +
  geom_histogram(aes(x=values),bins=30)

wind.allMeans = sapply(wind.all, mean)

wind.endRefIndex = ((2011-1982)*12)
#get the means corresponding to reference period
wind.refVal = wind.allMeans[1:wind.endRefIndex]#sapply(startMonth:endMonth, ninoAreaMean)

#calculate a baseline temperature for each month based on the reference period
wind.baselines = sapply(1:12,function(i) {
  monthValues = wind.refVal[seq(i,length(wind.refVal),12)]
  return(mean(monthValues))
})

#calculating anomaly (temperature for a given month - corresponding baseline)

wind.studyVal = wind.allMeans[(wind.endRefIndex+1):length(wind.allMeans)]
wind.studyAnom = wind.studyVal - rep(wind.baselines,(2023-2011))

wind.studyDF <- data.frame(wind.studyAnom, month=time.study)

ggplot(wind.studyDF) + 
  geom_point(aes(x=month, y = wind.studyAnom)) + 
  geom_line(aes(x=month, y = wind.studyAnom))

ggplot(wind.studyDF) + 
  geom_boxplot(aes(y=wind.studyAnom), width = 0.1) + 
  geom_half_point(aes(y = wind.studyAnom), side ="l",range_scale =0.3)

wind.opbins <- 1 + 3.322 * log(nrow(wind.studyDF))

ggplot(wind.studyDF)+
  geom_histogram(aes(x=wind.studyAnom),bins = wind.opbins)

#qqnorm(df_wind$periodAnomalies, pch = 1, frame = FALSE)
#qqline(df_wind$periodAnomalies, col = "steelblue", lwd = 2)

adf.test(wind.allDF$values) #if p < 0.05, reject, stationary
kpss.test(wind.allDF$values) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: stationary, differencing for wind


```

time shift for wind anomaly and sg sat anomaly
```{r time_shift_wind_anomaly}

cors = rep(0,13)
for (i in 0:12) {
  windanomaly = wind.studyAnom[1:(144-i)] #1:144 to 1:132
  sat_anom = sat.studyAnom[(1+i):(144)] #1:144 to 13:144
  cors[i]=cor(windanomaly,sat_anom)
}

corsDF = data.frame(cors = cors, x = 0:12)

ggplot(corsDF) +
  geom_line(aes(y=cors,x=x))

#should use the wind data to predict SG temperature x months in the future
#i.e. jan win data is used to predict x SG temp
wind.timeshift.sat = which.max(abs(cors))


```

```{r}

cors = rep(0,13)
for (i in 0:12) {
  windanomaly = wind.studyAnom[1:(144-i)] #1:144 to 1:132
  rainf_anom = rainf.studyAnom[(1+i):(144)] #1:144 to 13:144
  cors[i]=cor(windanomaly,rainf_anom)
}

corsDF = data.frame(cors = cors, x = 0:12)

ggplot(corsDF) +
  geom_line(aes(y=cors,x=x))

#same process as for sat
wind.timeshift.rainf = which.max(abs(cors))

```


## Reading Sub Surface Temp data
```{r}

months <- 01:12

years <- 1982:2022

substemp.folder <- 'data/subsurfacetempfiles/'

substemp.file <- paste0(substemp.folder, 'EN.4.2.2.f.analysis.g10.')

#extracting subsurface data from netcdf files
substemp <- lapply(1:length(years),function(i){
  
  monthly <- lapply(1:length(months),function(k){
    
    if(months[k]<10){
      data <- nc_open(paste(substemp.file,years[i],'0',months[k],'.nc',sep=''))
    }
    else{
      data <- nc_open(paste(substemp.file,years[i],months[k],'.nc',sep=''))
    }

    
  })
  
})

#finding the temp at each month and each year
substemp.all <- lapply(1:length(substemp),function(j){
  
  monthly <- lapply(1:length(months),function(n){
    
    #finding all the temperature data
    data.temp <- (ncvar_get(substemp[[j]][[n]],'temperature'))
    
    data.temp <- zoo::na.approx(data.temp)
    
    data.temp <- data.temp - 273.15
    
    #monthly temp in the area of interest, lon 190-240 (-170,-120), lat 78-88 (-5,5)
    all.temp <- data.temp[190:240,78:88]
    #temp <- data.temp[190:240,78:88,]
    
    })
    
})

substemp.all <- unlist(substemp.all,recursive=F)

substemp.all.plot <- unlist(substemp.all) #561924 NA values

#plotting timeseries for substemp
substemp.ts.plot <- data.frame(month = seq(as.Date('1982-01-01'),
                                           as.Date('2022-12-01'),
                                           by='month'),
                               substemp.all = substemp.all.plot)

substemp.ts <- ggplot(substemp.ts.plot)+
               geom_line(aes(x=(1:nrow(substemp.ts.plot)),y=substemp.all),linewidth=0.1)

substemp.ts

substemp.hist <- ggplot(data=data.frame(substemp.all.plot),aes(x=substemp.all.plot))+
                 geom_histogram(color='black', bins=50)+
                 ggtitle('Monthly Mean Sub Surf Temp in Pacific (1982-2022)')+
                 xlab('Mean Temp (deg C)')

substemp.hist

substemp.box <- boxplot(substemp.all.plot,
                             main='Monthly Mean Sub Surf Temp in Pacific (1982-2022)',
                             ylab = 'Mean Temp (deg C)')


#finding the mean temp at each month and each year
substemp.mean <- sapply(1:length(substemp),function(j){
  
  monthly <- sapply(1:length(months),function(n){
    
    #finding all the temperature data
    data.temp <- (ncvar_get(substemp[[j]][[n]],'temperature'))
    
    data.temp <- zoo::na.approx(data.temp)
    
    data.temp <- data.temp - 273.15
    
    #monthly temp in the area of interest, lon 190-240 (-170,-120), lat 78-88 (-5,5)
    all.temp <- mean(data.temp[190:240,78:88])
    #temp <- data.temp[190:240,78:88,]
    
    })
    
})

rm(substemp) #remove substemp to save space

substemp.refVal <- substemp.mean[,1:(2010-1982+1)] #29 columns for 29 years (1982-2010)

colnames(substemp.refVal) <- seq(1982,2010)

substemp.studyVal <- substemp.mean[,(2011-1982+1):ncol(substemp.mean)] #29 columns for 29 years (1982-2010)

colnames(substemp.studyVal) <- seq(2011,2022)

#calc baseline from refVal
substemp.baselines <- sapply(1:nrow(substemp.refVal),function(i){
  monthly.mean <- mean(substemp.refVal[i,])
})

#calc studyAnom
substemp.studyAnom <- c(substemp.studyVal-rep(substemp.baselines,(2022-2011+1)))

substemp.histAnom <- ggplot(data=data.frame(x=substemp.studyAnom),
                            aes(x=x))+
                 geom_histogram(color='black', bins=50)+
                 ggtitle('Monthly Sub Surf Temp Anomaly in Pacific (2011-2022)')+
                 xlab('Temp Anomalies (deg C)')

substemp.histAnom

substemp.boxAnom <- boxplot(substemp.studyAnom,
                             main='Monthly Sub Surf Temp Anomaly in Pacific (2011-2022)',
                             ylab = 'Temp Anomalies (deg C)')

adf.test(substemp.studyAnom) #if p < 0.05, reject, stationary
kpss.test(substemp.studyAnom) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case3: remove trend for substemp

```

## Correlate Sub Surface temp with SG Rainfall & SAT
```{r}
#set upper limit on data so reduce data length
df_plot2 <- data.frame(substemp_anom=substemp.studyAnom,
                      sat_anom=sat.studyAnom,
                      rainf_anom=rainf.studyAnom)

#calculate correlation
subs.sat.corr <- cor(df_plot2$substemp_anom,df_plot2$sat_anom)
subs.rainf.corr <- cor(df_plot2$substemp_anom,df_plot2$rainf_anom)

df_corr2 <- data.frame(subs.sat.corr,subs.rainf.corr)

print(df_corr2)

#scatter plots
ggplot(df_plot2,aes(x=rainf_anom))+
  geom_point(aes(y=substemp_anom,color='subs.rainf'))+
  scale_color_manual(values=c('subs.rainf'='pink'))+
  ggtitle('Monthly Sub Surf Temp anomaly againt SG Rainfall from Jan 2011- Dec 2022')

ggplot(df_plot2,aes(x=sat_anom))+
  geom_point(aes(y=substemp_anom,color='subs.sat'))+
  scale_color_manual(values=c('subs.sat'='lightblue'))+
  ggtitle('Monthly Sub Surf Temp anomaly againt SG SAT anomaly from Jan 2011- Dec 2022')

ggplot(df_plot2,aes(x=1:nrow(df_plot2)))+
  geom_line(aes(y=substemp_anom,color='subs'))+
  geom_line(aes(y=sat_anom,color='sat'))+
  #geom_line(aes(y=rainf_anom,color='rainf'))+
  scale_color_manual(values=c(subs='red',sat='lightgreen','rainf'='lightblue'))

# fakesat <- df_plot2$sat_anom[5:nrow(df_plot2)]
# fakesub <- df_plot2$substemp_anom[1:(nrow(df_plot2)-4)]
# 
# ggplot()+
#   geom_line(aes(x=1:140,y=fakesub,color='subs'))+
#   geom_line(aes(x=1:140,y=fakesat,color='sat'))+
#   scale_color_manual(values=c(subs='red',sat='lightgreen','rainf'='lightblue'))

# timeshifting to find best correlation for substemp & sg sat, consider only forward shift since we using pacific data to predict sg data
corr.ts.sat.substemp <- sapply(1:12,function(i){
  
  ts.substemp <- substemp.studyAnom[i:length(substemp.studyAnom)]
  ts.sat <- sat.studyAnom[1:(length(sat.studyAnom)-i+1)]
  cor.ts <- cor(ts.substemp,ts.sat)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.sat.substemp.df <- data.frame(month = corr.ts.sat.substemp[1,],
                              corr.substemp.sat = corr.ts.sat.substemp[2,])

substemp.timeshift.sat <- which.max(abs(corr.ts.sat.substemp.df$corr.substemp.sat))-1

# timeshifting to find best correlation for substemp & sg rainf, consider only forward shift since we using pacific data to predict sg data
corr.ts.rainf.substemp <- sapply(1:12,function(i){
  
  ts.substemp <- substemp.studyAnom[i:length(substemp.studyAnom)]
  ts.rainf <- rainf.studyAnom[1:(length(rainf.studyAnom)-i+1)]
  cor.ts <- cor(ts.substemp,ts.rainf)
  month.cor <- c(i,cor.ts)
  return (month.cor)
})

corr.ts.rainf.substemp.df <- data.frame(month = corr.ts.rainf.substemp[1,],
                              corr.substemp.rainf = corr.ts.rainf.substemp[2,])

substemp.timeshift.rainf <- which.max(abs(corr.ts.rainf.substemp.df$corr.substemp.rainf))
```

```{r}

nc_data <- nc_open("data/olr-monthly_v02r07_197901_202303.nc")

#print (nc_data)

#attributes(nc_data$var)

#attributes(nc_data$dim)

lat <- ncvar_get(nc_data, "lat")

nlat <- dim(lat)

lon <- ncvar_get(nc_data, "lon")

nlon <- dim(lon)

time <- ncvar_get(nc_data, "time")

head(time) 
tunits <- ncatt_get(nc_data, "time", "units") 
nt <- dim(time) 

cloudiness <- ncvar_get(nc_data, "olr") 

fillvalue <- ncatt_get(nc_data, "olr", "_FillValue")

dim(cloudiness)

cloudiness[cloudiness==fillvalue$value] <- NA


#Obs for cloudiness variable: boxplot, histogram, qqplot


# Convert the number of hours to POSIXct format, specifying the origin as "1800-01-01"
datetime <- as.Date(time, origin="1979-01-01")

lat_range <- c(-5, 5)
lon_range <- c(190, 240)

lat_idx <- which(lat >= lat_range[1] & lat <= lat_range[2])
lon_idx <- which(lon >= lon_range[1] & lon <= lon_range[2])

id.all <- which(datetime >= '1982-01-01' & datetime < '2023-01-1')
okla.all <- cloudiness[lon_idx,lat_idx,id.all]

okla.all.mean <- sapply(1:dim(okla.all)[3],function(i){
  val <- mean(okla.all[,,i])
})

okla.all.mean <- data.frame(okla = okla.all.mean,
                            month = seq(as.Date('1982-01-01'),
                                         as.Date('2022-12-01'),
                                         by='month'))
colnames(okla.all.mean) <- c('okla','month')

# plotting hist and box for okla values
okla.hist <- ggplot(data=data.frame(x=c(okla.all)),aes(x=x))+
  geom_histogram(color='black',bins=50)+
  ggtitle('Monthly OLR in the pacific (1980-2022)')
okla.hist

okla.box <- boxplot(c(okla.all),
                    main='Monthly OLR in the Pacific (1980-2022)',
                    xlab = 'OLR')

id.ref <- which(datetime >= '1982-01-01' & datetime < '2011-01-1')
okla.refVal <- sapply(1:(length(id.ref)/12),function(i){
  monthly <- sapply(1:12,function(j){
    val <- mean(cloudiness[lon_idx,lat_idx,i*j])
  })
})

id.study <- which(datetime >= '2011-01-01' & datetime < '2023-01-1')
okla.studyVal <- sapply(1:length(id.study),function(i){
  val <- mean(cloudiness[lon_idx,lat_idx,i])
})

#get baseline values
okla.baselines <- sapply(1:12,function(i){
  val <- mean(okla.refVal[i,])
})

#get anom values
okla.studyAnom <- okla.studyVal - rep(okla.baselines,144/12)

#plotting hist and box of okla anomalies
okla.histAnom <- ggplot(data=data.frame(okla.studyAnom),aes(x=okla.studyAnom))+
  geom_histogram(color='black',bins=50)+
  ggtitle('Monthly OLR anomaly in the pacific (1980-2022)')+
  xlab('OLR Anomalies')
okla.histAnom

okla.boxAnom <- boxplot(c(okla.all),
                    main='Monthly OLR anomaly in the Pacific (1980-2022)',
                    xlab = 'OLR Anomalies')

# cloud_data <- cloudiness[lon_idx,lat_idx,]
# 
# lat_1 <- data.frame(t(cloud_data[1,,]), datetime)
# 
# ggplot(lat_1)+
#   geom_point(aes(x=datetime, y=X1))+
#   geom_line (aes(x=datetime, y=X1))
# 
# pacific_avg <- sapply(1:531, function(a){
#   year_avg <- mean(cloudiness[,,a])
# })
# 
# lat <- data.frame(pacific_avg, datetime)

```

```{r}

ggplot(okla.all.mean)+
  geom_point(aes(x=month, y=okla))+
  geom_line (aes(x=month, y=okla))+
  labs(x= 'Time (Years)', y = 'OLR'~W/m^2) +
  theme_minimal() +
  ggtitle("Plot showing OLR against Time")

adf.test(okla.studyAnom) #if p < 0.05, reject, stationary
kpss.test(okla.studyAnom) #if p < 0.05, reject, not stationary

#Case 1: Both tests conclude that the series is not stationary -> series is not stationary
#Case 2: Both tests conclude that the series is stationary -> series is stationary
#Case 3: KPSS = stationary and ADF = not stationary  -> trend stationary, remove the trend to make series strict stationary
#Case 4: KPSS = not stationary and ADF = stationary -> difference stationary, use differencing to make series stationary

# Case4: stationary, differencing for wind

```

```{r}

ggplot(okla.all.mean)+
  geom_boxplot(aes(y=okla), width = 0.1) + 
  geom_half_point(aes(y = okla), side ="l",range_scale =0.3)+
  labs(y = 'OLR'~W/m^2) +
  theme_minimal() +
  ggtitle("Raincloud plot showing the distribution of \nData points of OLR")+ 
  theme(axis.title.x=element_blank(),
        axis.text.x=element_blank(),
        axis.ticks.x=element_blank())

```


```{r}

# opbins_lat <- 1 + 3.322 * log(nrow(lat))
# 
# 
# test <- lat[385:528,]
# 
# ggplot(lat)+
#   geom_histogram(aes(x=pacific_avg),bins = 50)+
#   labs(x = 'OLR'~W/m^2, y='Frequency') +
#   theme_minimal() +
#   ggtitle("Histogram showing the distribution of \nData points of OLR")

```

```{r}


# #qqnorm(lat$pacific_avg, pch = 1, frame = FALSE)
# #qqline(lat$pacific_avg, col = "steelblue", lwd = 2)
# reference <- lat[13:384,]
# 
# monthly_means = sapply(1:12, function(i) {
#   mean(reference[seq(i,nrow(reference),12),1])
# })
# 
# test <- lat[385:528,]
# 
# test_anomoly = test[,1] - rep(monthly_means,12)
# 
# hist(test_anomoly)


```
From the boxplot, histogram, and qqplot, we can observe a distinct right skewed distribution of the datapoints, indicating the presence of very large outliers. We can consider excluding these points or subjecting the dataset to a log, sqrt, or normalize transformation. 



## All Data, Pairs Plot (Not Time Shifted)
```{r message=FALSE, warning=FALSE}
enso.df <- data.frame(month = time.study, 
                      sat.sg=sat.studyAnom, # Normally distributed
                      rainf.sg=rainf.studyAnom, #
                      sst.pcf=sst.studyAnom,
                      substemp.pcf=substemp.studyAnom,
                      okla.pcf=okla.studyAnom,
                      wind.pcf=wind.studyAnom)

enso.df.nd <- enso.df[,2:ncol(enso.df)]

lowerFn <- function(data, mapping, method = "lm", ...) {
  p <- ggplot(data = data, mapping = mapping) +
    geom_point(colour = "black",size=0.3,alpha=0.5) +
    geom_smooth(method = method, color = "red", alpha=0.7,linewidth=0.5,...)
  p
}

ggpairs(enso.df.nd,columns = 1:ncol(enso.df.nd),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))

#Stat tests
enso.df.t1 <- enso.df[which(enso.df$month < '2017-01-01'),] #half of data
enso.df.t2 <- enso.df[which(enso.df$month >='2017-01-01'),]

#t test for anomaly
t.test.results <- sapply(2:ncol(enso.df.t1),function(i){
  
  t.test(enso.df.t1[,i],enso.df.t2[,i],paired=F)})

colnames(t.test.results) <- colnames(enso.df.nd)

#ANOVA
#all data for 1982-2021 only (40 years).
all.data <- data.frame(month = time.all,
                      sat.sg=c(sat.refVal,sat.studyVal),
                      rainf.sg=c(rainf.refVal,rainf.studyVal),
                      sst.pcf=c(sst.refVal,sst.studyVal),
                      substemp.pcf=c(substemp.refVal,substemp.studyVal),
                      okla.pcf=c(okla.refVal,okla.studyVal),
                      wind.pcf=c(wind.refVal,wind.studyVal))

all.data <- all.data[1:480,]

all.data$decade <- factor(c(rep(1,120),
                         rep(2,120),
                         rep(3,120),
                         rep(4,120)))

var.sat<-leveneTest(sat.sg~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.sat <- summary(aov(sat.sg ~ decade, data=all.data))
p.sat <- aov.sat[[1]]$`Pr(>F)`[1]

var.rainf<-leveneTest(rainf.sg~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.rainf <- summary(aov(rainf.sg ~ decade, data=all.data))
p.rainf <- aov.rainf[[1]]$`Pr(>F)`[1]

var.sst<-leveneTest(sst.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.sst <- summary(aov(sst.pcf ~ decade, data=all.data))
p.sst <- aov.sst[[1]]$`Pr(>F)`[1]

var.substemp<-leveneTest(substemp.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.substemp <- summary(aov(substemp.pcf ~ decade, data=all.data))
p.substemp <- aov.substemp[[1]]$`Pr(>F)`[1]

var.okla<-leveneTest(okla.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.okla <- summary(aov(okla.pcf ~ decade, data=all.data))
p.okla <- aov.okla[[1]]$`Pr(>F)`[1]

var.wind<-leveneTest(wind.pcf~decade,data=all.data) #p > 0.05 so var are same (H0: var is same)
aov.wind <- summary(aov(wind.pcf ~ decade, data=all.data))
p.wind <-aov.wind[[1]]$`Pr(>F)`[1]

# since p values of sat, sst, wind <0.05, we cannot rej H0. ie the means are different across decadal time scales. Therefore, in order to analyse enso trend, we will need more data to capture the variations better as a sample size of 5 decades are clearly insufficient to fully capture the variations.

```

# Correlating cloudiness anomaly of 2011-2023 to Singapore anomaly (Rainfall and temeprature)
#``r
#cor(rainf.anom[,2], test_anomoly)
#cor(sat.anom[,2], test_anomoly)
#cors = rep(0,11)

#for (i in -12:0) {
#  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
#                        sat_anom = sat.anom$anom[(-i+1):nrow(sat.anom)])
#  cors[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$sat_anom))
#}
#
#cors2 = rep(0,11)
#for (i in -12:0) {
#  df_plot <- data.frame(cloudanomaly = test_anomoly[1:(length(test_anomoly)+i)],
#                        rain_anom = rainf.anom$anom[(-i+1):nrow(rainf.anom)])
#  cors2[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$rain_anom))
#}



## Correlating cloudiness anomaly of 2011-2023 to Singapore anomaly (Rainfall and temeprature)

```{r}

cor(rainf.studyAnom, okla.studyAnom)

cor(sat.studyAnom, okla.studyAnom)

cors = rep(0,11)
for (i in -12:0) {
  df_plot <- data.frame(cloudanomaly = okla.studyAnom[1:(length(okla.studyAnom)+i)],
                        sat_anom = sat.studyAnom[(-i+1):length(sat.studyAnom)])
  cors[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$sat_anom))
}

okla.timeshift.sat = which.max(abs(cors))

cors2 = rep(0,11)
for (i in -12:0) {
  df_plot <- data.frame(cloudanomaly = okla.studyAnom[1:(length(okla.studyAnom)+i)],
                        rain_anom = rainf.studyAnom[(-i+1):length(rainf.studyAnom)])
  cors2[i+11]=cor(as.numeric(df_plot$cloudanomaly),as.numeric(df_plot$rain_anom))
}

okla.timeshift.rainf = which.max(abs(cors2))

```
## Timeshifting our data

```{r message=FALSE, warning=FALSE}

#enso.df

maxshift = max(c(wind.timeshift.rainf,
                 wind.timeshift.sat,
                 substemp.timeshift.rainf,
                 #best.ts.substemp.sat,
                 okla.timeshift.sat,
                 okla.timeshift.rainf))

n = nrow(enso.df)

rainf.timeshift.df = data.frame(rainf.sg = enso.df$rainf.sg[(1+maxshift):n],
                                sst.pcf = enso.df$sst.pcf[(1+maxshift):n],#not timeshifted
                                substemp.pcf = enso.df$substemp.pcf[(1+(maxshift-substemp.timeshift.rainf)):(n-substemp.timeshift.rainf)],
                                okla.pcf = enso.df$okla.pcf[(1+maxshift-okla.timeshift.rainf):(n-okla.timeshift.rainf)],
                                wind.pcf = enso.df$wind.pcf[(1+maxshift-wind.timeshift.rainf):(n-wind.timeshift.rainf)])

                                
sat.timeshift.df = data.frame(sat.sg = enso.df$sat.sg[(1+maxshift):n],
                              sst.pcf = enso.df$sst.pcf[(1+maxshift):n],#not timeshifted,
                              substemp.pcf = enso.df$substemp.pcf[(1+maxshift):n],#not timeshifted
                              okla.pcf = enso.df$okla.pcf[(1+maxshift-okla.timeshift.sat):(n-okla.timeshift.sat)],
                              wind.pcf = enso.df$wind.pcf[(1+maxshift-wind.timeshift.sat):(n-wind.timeshift.sat)])
                              
ggpairs(sat.timeshift.df,columns = 1:ncol(sat.timeshift.df),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))


ggpairs(rainf.timeshift.df,columns = 1:ncol(rainf.timeshift.df),
        upper = list(continuous = 'cor', combo = "box_no_facet"),
        lower = list(continuous = wrap(lowerFn, method='lm')),
        diag = list(continuous = wrap('barDiag',color='black',bins=25)),
        title='Distribution and Correlation of Predictor Variables')+
        theme(axis.text.x = element_text(angle=90),
              axis.text = element_text(size=7))

```


```{r, eval=FALSE}


rainFormula = (rainf.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)
satFornula = (sat.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

testIndices = sample(1:136,30)

train = rainf.timeshift.df[-testIndices,]
test = rainf.timeshift.df[testIndices,]

train.st = data.frame(scale(train))

means = colMeans(train); sds = apply(train,2,sd)

test.st = data.frame(scale(test,center=means,scale=sds))

model = glm(formula=rainFormula,
    data=train.st)

preds = predict(model,test.st)


with(summary(model), 1 - deviance/null.deviance)

df = data.frame(preds = preds,
           obs = test$rainf.sg)


#parallelize training of multiple models (initial weights are randomized)
numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','train.st'))
nets = parLapply(cl,1:(36), function(i) {
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
})
stopCluster(cl)

rsquares = sapply(nets[1:(length(nets))], function(net) {
  pred = predict(object=net,newdata=test.st)[,1]
  resi = test.st$rainf.sg-pred
  SSR = sum(resi^2)
  resi2 = test.st['rainf.sg']-mean(test.st$rainf.sg)
  SST = sum(resi2^2)
  rsquare = 1-SSR/SST
})

round(rsquares,4)



#rsquarer <- function (x, y) cor(x, y) ^ 2

#rsquarer(pred,test.st$rainf.sg)
```


```{r, EVAL=F}

rsquarer = function(obs,pred) {
  tss = sum((obs-mean(obs))^2)
  rss = sum((obs-pred)^2)
  return(1-rss/tss)
}

params <- list(
  objective = "reg:squarederror", # Specify the objective (e.g., regression)
  eta = 0.01, # Learning rate
  max_depth = 5, # Maximum depth of trees
  subsample = 0.8, # Subsampling ratio
  colsample_bytree = 0.8 # Column subsampling ratio
)

rainFormula = (rainf.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

k=5
n=nrow(timeshiftedRainfDF)
groups = cut(1:n,k,label=F)
#timeshiftedRainfDF = timeshiftedRainfDF[sample(n),]

numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','timeshiftedRainfDF','groups','rsquarer','xgboost','params','randomForest'))
results = parSapply(cl, 1:k, function(i) {
  
  train = timeshiftedRainfDF[groups!=i,]
  test = timeshiftedRainfDF[groups==i,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=rainFormula,data=train.st)
  pred = predict(model,test.st)
  linearRMSE = sqrt(sum((pred-test.st$rainf.sg)^2)/length(test.st))
  linearRSQ = rsquarer(test.st$rainf.sg,pred)
  
  # net = neuralnet(formula=rainFormula,
  #                 data=train.st,
  #                 #act.fct='logistic',
  #                 hidden=c(6),
  #                 algorithm="sag",
  #                 stepmax=1e+08,
  #                 linear.output=FALSE)
  # 
  # pred = predict(object=net,newdata=test.st)[,1]
  # nnRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # nnRSQ = rsquarer(test.st$rainf.sg,pred)
  # 
  # xgb = xgboost(data = as.matrix(train.st[,2:5]), label = train.st[,1], params = params,  nrounds = 100)
  # pred = predict(xgb,as.matrix(test.st[,2:5]))
  # xgbRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # xgbRSQ = rsquarer(test.st$rainf.sg,pred)
  
  # rf = randomForest(rainFormula,data=train.st)
  # pred = predict(rf,newdata=test.st)
  # rfRMSE = sqrt(sum((pred-test.st$rainf.sg)^2/length(test.st)))
  # rfRSQ = rsquarer(test.st$rainf.sg,pred)

  return(c(linearRMSE,linearRSQ))#,nnRMSE,nnRSQ,rfRMSE,rfRSQ,xgbRMSE,xgbRSQ))
})

results = data.frame(t(results))
colnames(results) = c('linearrmse','linearrsq')#,'nnrmse','nnrsq','rfRMSE','rfRSQ','xgbRMSE','xgbRSQ')
colMeans(results)

```


```{r, eval=FALSE}

satFormula = (sat.sg ~ sst.pcf*substemp.pcf+okla.pcf+wind.pcf)

#timeshiftedSATDF = timeshiftedSATDF[sample(n),]
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','satFormula','timeshiftedSATDF','groups','randomForest','xgboost','params','rsquarer'))
results = parSapply(cl, 1:k, function(i) {
  train = timeshiftedSATDF[groups!=i,]
  test = timeshiftedSATDF[groups==i,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=satFormula,data=train.st)
  pred = predict(model,test.st)
  #linearRMSE = sqrt((pred-test.st$sat.sg)^2)
  linearRMSE = sqrt(sum((pred-test.st$sat.sg)^2)/length(test.st))
  linearRSQ = rsquarer(test.st$sat.sg,pred)
  #linearResult = with(summary(model), 1 - deviance/null.deviance)
  
  net = neuralnet(formula=satFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
  
  pred = predict(object=net,newdata=test.st)[,1]
  nnRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  nnRSQ = rsquarer(test.st$sat.sg,pred)
  
  xgb = xgboost(data = as.matrix(train.st[,2:5]), label = train.st[,1], params = params,  nrounds = 100)
  pred = predict(xgb,as.matrix(test.st[,2:5]))
  xgbRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  xgbRSQ = rsquarer(test.st$sat.sg,pred)
  
  rf = randomForest(satFormula,data=train.st)
  pred = predict(rf,newdata=test.st)
  rfRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  rfRSQ = rsquarer(test.st$sat.sg,pred)
  # 
  # rnn = trainr(Y=train.st[,1],X=train.st[,2:5],learningrate=1)
  # pred = predictr(rnn,X=test.st[,2:5])
  # rfRMSE = sqrt(sum((pred-test.st$sat.sg)^2/length(test.st)))
  # rfRSQ = rsquarer(test.st$sat.sg,pred)
  

  return(c(linearRMSE,linearRSQ,nnRMSE,nnRSQ,rfRMSE,rfRSQ,xgbRMSE,xgbRSQ))
})

results = data.frame(t(results))
colnames(results) = c('linearrmse','linearrsq','nnrmse','nnrsq','rfRMSE','rfRSQ','xgbRMSE','xgbRSQ')
colMeans(results)

```
## Buiilding a Neural Net
```{r, eval=FALSE}

library(parallel)
library(neuralnet)

rainFormula = (rainf.sg ~ sst.pcf+substemp.pcf+okla.pcf+wind.pcf)
satFormula = (sat.sg ~ sst.pcf+substemp.pcf+okla.pcf+wind.pcf)

train = timeshiftedRainfDF[1:100,]
test = timeshiftedRainfDF[101:136,]

train.st = data.frame(scale(train))

means = colMeans(train); sds = apply(train,2,sd)

test.st = scale(test,center=means,scale=sds)

model = glm(formula=rainFormula,
    data=train)

preds = predict(model,test)

df = data.frame(preds = preds,
           obs = test$rainf.sg)


#parallelize training of multiple models (initial weights are randomized)
numCores = detectCores()
cl = makeCluster(numCores)
clusterExport(cl,c('neuralnet','rainFormula','train.st'))
nets = parLapply(cl,1:(12), function(i) {
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=5,
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=F)
})
stopCluster(cl)

test.st <- data.frame(test.st)

rsq.fn <- function (x, y) cor(x, y) ^ 2

rsq.net = sapply(nets[1:(length(nets))], function(net) {
  pred = predict(object=net,newdata=test.st)[,1]
  rsq.fn(test.st$rainf.sg,pred)
  
})


rsq.glm <- rsq1(test.st$rainf.sg,preds)


train = timeshiftedRainfDF[groups!=1,]
  test = timeshiftedRainfDF[groups==1,]
  
  train.st = data.frame(scale(train))
  means = colMeans(train); sds = apply(train,2,sd)
  test.st = data.frame(scale(test,center=means,scale=sds))
  
  model = glm(formula=rainFormula,data=train.st)
  pred = predict(model,test.st)
  linearRMSE = sqrt(sum((pred-test.st$rainf.sg)^2)/length(test.st))
  
  net = neuralnet(formula=rainFormula,
                  data=train.st,
                  #act.fct='logistic',
                  hidden=c(4,4),
                  algorithm="sag",
                  stepmax=1e+08,
                  linear.output=FALSE)
  
  pred = predict(object=net,newdata=test.st)[,1]



Y = array(,dim=c(6,129,1))
Y[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),1]))

rnn = trainr(Y=Y,X=X,learningrate=1)
pred = predictr(rnn,X=X)


X = array(,dim=c(6,129,1))
X[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),2]))

Y = array(,dim=c(6,129,1))
Y[,,1] = sapply(start_indexes, function(i) return(timeshiftedRainfDF[i:(i+max_len-1),1]))

rnn = trainr(Y=Y,X=X,learningrate=1,numepochs=10)
pred = predictr(rnn,X=X)



```


```{r recurrentneuralnet, EVAL=F}

## RNN

unshiftedSATDF = data.frame(sat.sg = enso.df$sat.sg,
                              sst.pcf = enso.df$sst.pcf,#not timeshifted,
                              substemp.pcf = enso.df$substemp.pcf,#not timeshifted
                              okla.pcf = enso.df$okla.pcf,
                              wind.pcf = enso.df$wind.pcf)

  
max_len <- 2 # the number of previous examples we'll look at
n = nrow(unshiftedSATDF)

start_indexes = seq(1,n-max_len+1,by=1)

X1 = t(sapply(start_indexes, function(i) return(unshiftedSATDF[i:(i+max_len-1),2])))
X2 = t(sapply(start_indexes, function(i) return(unshiftedSATDF[i:(i+max_len-1),3])))
X3 = t(sapply(start_indexes, function(i) return(unshiftedSATDF[i:(i+max_len-1),4])))
X4 = t(sapply(start_indexes, function(i) return(unshiftedSATDF[i:(i+max_len-1),5])))
X = data.frame(cbind(X1,X2,X3,X4))

Y = data.frame(Y=unshiftedSATDF[max_len:n,1])

df = cbind(Y,X)

train = df[1:100,]
test = df[101:131,]
  
train.st = data.frame(scale(train))
means = colMeans(train); sds = apply(train,2,sd)
test.st = data.frame(scale(test,center=means,scale=sds))

net = neuralnet(formula = Y~.,
          data = df,
          hidden=c(6,6),
          algorithm="sag",
          stepmax=1e+08,
          linear.output=TRUE)

pred = predict(object=net,newdata=test.st)[,1]

compare = data.frame(pred=pred,obs=test$Y)

rnnRMSE = sqrt(sum((pred-test.st$Y)^2/length(test.st)))
rnnRSQ = rsquarer(test.st$Y,pred)

rnnRMSE;rnnRSQ

## RNN END

```

```{r recurrentneuralnet, EVAL=F}

## RNN

  
max_len <- 3 # the number of previous examples we'll look at
n = nrow(timeshiftedSATDF)

start_indexes = seq(1,n-max_len+1,by=1)

X1 = t(sapply(start_indexes, function(i) return(timeshiftedSATDF[i:(i+max_len-1),2])))
X2 = t(sapply(start_indexes, function(i) return(timeshiftedSATDF[i:(i+max_len-1),3])))
X3 = t(sapply(start_indexes, function(i) return(timeshiftedSATDF[i:(i+max_len-1),4])))
X4 = t(sapply(start_indexes, function(i) return(timeshiftedSATDF[i:(i+max_len-1),5])))
X = data.frame(cbind(X1,X2,X3,X4))

Y = data.frame(Y=timeshiftedSATDF[max_len:n,1])

df = cbind(Y,X)

train = df[1:100,]
test = df[101:131,]
  
train.st = data.frame(scale(train))
means = colMeans(train); sds = apply(train,2,sd)
test.st = data.frame(scale(test,center=means,scale=sds))

net = neuralnet(formula = Y~.,
          data = df,
          hidden=c(6),
          algorithm="sag",
          stepmax=1e+08,
          linear.output=TRUE)

pred = predict(object=net,newdata=test.st)[,1]

compare = data.frame(pred=pred,obs=test$Y)

rnnRMSE = sqrt(sum((pred-test.st$Y)^2/length(test.st)))
rnnRSQ = rsquarer(test.st$Y,pred)

rnnRMSE;rnnRSQ

## RNN END
```

```{r enso_test}

#monthly data from 1/1979 to 03/2023
ensoIndex = read.csv('data/meiv2.csv',sep=",",header=F)
startIndex = (1982-1979)*12+1; endIndex = (2023-1979)*12

#i want only 1/1982 to 12/2022
ensoIndex = ensoIndex[startIndex:endIndex]

ensoPos = ensoIndex>=0
ensoNeg = ensoIndex<0


#find enso variable: split sg rainfall and SAT into 2 groups based on enso>0 or enso<0

ensoPos_sgRainf = rainf.studyAnom[ensoPos]
ensoNeg_sgRainf = rainf.studyAnom[ensoNeg]

ensoPos_sgSAT = sat.studyAnom[ensoPos]
ensoNeg_sgSAT = sat.studyAnom[ensoNeg]

#then welch 2-sample test to see if got effect
var.test(ensoPos_sgRainf, ensoNeg_sgRainf)

#p-value ~0.5, can be considered same variance?
t.test(ensoPos_sgRainf, ensoNeg_sgRainf,var.equal=TRUE)

#then welch 2-sample test to see if got effect
var.test(ensoPos_sgSAT, ensoNeg_sgSAT)

#p-value ~0.5, can be considered same variance?
t.test(ensoPos_sgSAT, ensoNeg_sgSAT,var.equal=FALSE)



# 
# 
# ensoPos = ensoIndex>=0.3
# ensoNeg = ensoIndex<=-0.3
# 
# 
# #find enso variable: split sg rainfall and SAT into 2 groups based on enso>0 or enso<0
# 
# ensoPos_sgRainf = rainf.anom$anom[ensoPos]
# ensoNeg_sgRainf = rainf.anom$anom[ensoNeg]
# 
# ensoPos_sgSAT = sat.anom$anom[ensoPos]
# ensoNeg_sgSAT = sat.anom$anom[ensoNeg]
# 
# # histogram(ensoPos_sgRainf)
# 
# 
# #then welch 2-sample test to see if got effect
# var.test(ensoPos_sgRainf, ensoNeg_sgRainf)
# 
# #p-value ~0.5, can be considered same variance?
# t.test(ensoPos_sgRainf, ensoNeg_sgRainf,var.equal=FALSE)
# 
# 
# #then welch 2-sample test to see if got effect
# var.test(ensoPos_sgSAT, ensoNeg_sgSAT)
# 
# #p-value ~0.5, can be considered same variance?
# t.test(ensoPos_sgSAT, ensoNeg_sgSAT,var.equal=FALSE)

```

